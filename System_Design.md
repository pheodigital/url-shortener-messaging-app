# ðŸ§  System Design â€” URL Shortener + Analytics Platform

This document covers the complete system design decisions: why we chose each component, what tradeoffs we made, and how the system behaves under load.

---

## 1. Problem Statement

### Functional Requirements
- User can create a short URL from a long URL
- Anyone can visit the short URL and get redirected to the original
- Users must be authenticated (Google OAuth2) to create URLs
- Dashboard shows analytics: total clicks, per URL stats, location, device
- Users can delete their own URLs

### Non-Functional Requirements
- Redirect must be **fast** (< 10ms p99 ideally, cache-first)
- System must handle **high read traffic** (redirects >> writes)
- Analytics can be **eventually consistent** (slight delay is acceptable)
- Auth must be **stateless** (servers can scale horizontally)
- System must be **observable** (logs, metrics, traces)

---

## 2. Capacity Estimation

### Assumptions (for learning context)
```
Daily active users         â†’  10,000
URLs created per day       â†’  5,000
Redirects per day          â†’  500,000   (100x write ratio)
Analytics events per day   â†’  500,000   (1 per redirect)
```

### Read/Write Ratio
```
Reads (redirects)  : Writes (create URL) = 100 : 1

This tells us:
â†’ Optimize heavily for READ path
â†’ Cache is not optional, it's critical
â†’ Write path can afford slightly more latency
```

### Storage Estimation
```
PostgreSQL (URLs table):
  Each row â‰ˆ 500 bytes
  5,000 URLs/day Ã— 365 days = 1.8M rows/year
  Storage: ~900 MB/year â†’ very manageable

Redis (Cache):
  Each entry â‰ˆ 200 bytes (shortcode + longURL)
  Cache top 100K URLs â†’ ~20 MB â†’ negligible

MongoDB (Click events):
  Each event â‰ˆ 300 bytes
  500,000 events/day Ã— 365 = 182M events/year
  Storage: ~55 GB/year â†’ needs indexing strategy
```

---

## 3. API Design

### Auth Service (PORT 3001)

```
GET  /auth/google              â†’ Redirect to Google OAuth2
GET  /auth/google/callback     â†’ Handle OAuth2 callback, issue JWT
POST /auth/logout              â†’ Blacklist JWT in Redis
GET  /auth/me                  â†’ Return current user info
GET  /health                   â†’ Health check
```

### URL Service (PORT 3002)

```
POST   /api/urls               â†’ Create short URL (authenticated)
GET    /api/urls               â†’ List my URLs (authenticated)
DELETE /api/urls/:id           â†’ Delete a URL (authenticated)
GET    /:shortcode             â†’ Redirect to long URL (public, HOT PATH)
GET    /health                 â†’ Health check
```

### Analytics Service (PORT 3003)

```
GET  /api/analytics/:shortcode       â†’ Stats for one URL
GET  /api/analytics/dashboard        â†’ Aggregated stats for user
GET  /health                         â†’ Health check
```

---

## 4. Database Design

### PostgreSQL Schema

```sql
-- Users table
users
â”œâ”€â”€ id          UUID PRIMARY KEY
â”œâ”€â”€ email       VARCHAR UNIQUE NOT NULL
â”œâ”€â”€ name        VARCHAR
â”œâ”€â”€ googleId    VARCHAR UNIQUE
â”œâ”€â”€ createdAt   TIMESTAMP DEFAULT NOW()
â””â”€â”€ updatedAt   TIMESTAMP

-- URLs table
urls
â”œâ”€â”€ id          UUID PRIMARY KEY
â”œâ”€â”€ shortcode   VARCHAR(10) UNIQUE NOT NULL  â† indexed
â”œâ”€â”€ longUrl     TEXT NOT NULL
â”œâ”€â”€ userId      UUID REFERENCES users(id)
â”œâ”€â”€ isActive    BOOLEAN DEFAULT TRUE
â”œâ”€â”€ createdAt   TIMESTAMP DEFAULT NOW()
â””â”€â”€ updatedAt   TIMESTAMP

-- Indexes
CREATE INDEX idx_urls_shortcode ON urls(shortcode);   â† critical for redirect
CREATE INDEX idx_urls_userId ON urls(userId);
```

### MongoDB Schema (Analytics)

```javascript
// click_events collection
{
  _id: ObjectId,
  shortcode: String,      // indexed
  longUrl: String,
  userId: String,
  timestamp: Date,        // indexed
  ip: String,
  userAgent: String,
  country: String,
  city: String,
  device: String,         // mobile / desktop / tablet
  browser: String,
  referrer: String
}

// Indexes
{ shortcode: 1 }
{ shortcode: 1, timestamp: -1 }
{ userId: 1, timestamp: -1 }
```

### Redis Key Design

```
# URL Cache (TTL: 24 hours)
url:{shortcode}  â†’  "https://original-long-url.com"

# JWT Blacklist (TTL: token expiry time)
blacklist:{jti}  â†’  "1"

# Rate Limiting (TTL: 60 seconds)
ratelimit:{ip}   â†’  "count"

# Session (TTL: 7 days)
session:{userId} â†’  { email, name, ... }
```

---

## 5. The Hot Path â€” Redirect Flow

This is the most critical flow. It must be as fast as possible.

```
User hits: GET /abc123
              â”‚
              â–¼
         NGINX Gateway
         (rate limit check â†’ 429 if exceeded)
              â”‚
              â–¼
         URL Service
              â”‚
              â”œâ”€â”€â†’ Redis Cache HIT?
              â”‚         â”‚
              â”‚         YES â†’ Return 302 redirect immediately
              â”‚               + publish click event to RabbitMQ (async)
              â”‚               Total time: ~2-5ms
              â”‚
              â””â”€â”€â†’ Cache MISS
                        â”‚
                        â–¼
                  Query PostgreSQL
                  (SELECT longUrl WHERE shortcode = 'abc123')
                        â”‚
                        â–¼
                  Store in Redis (TTL 24h)
                        â”‚
                        â–¼
                  Return 302 redirect
                  + publish click event to RabbitMQ (async)
                  Total time: ~20-50ms
```

**Why fire-and-forget to RabbitMQ?**
Because analytics are non-critical. If we waited for MongoDB to write before redirecting, we'd add 20-100ms to every single redirect. That's unacceptable.

---

## 6. Caching Strategy

### Cache-Aside Pattern (what we use)

```
Read:
  1. Check Redis first
  2. If HIT â†’ return value
  3. If MISS â†’ read from PostgreSQL â†’ write to Redis â†’ return value

Write (create URL):
  1. Write to PostgreSQL (source of truth)
  2. Optionally pre-warm cache (write to Redis too)

Delete (URL removed):
  1. Delete from PostgreSQL
  2. DELETE the Redis key immediately (cache invalidation)
```

### Why Cache-Aside and not Write-Through?
Write-through would cache everything on write. Since most URLs may never be visited, we'd waste Redis memory. Cache-aside only caches what's actually requested.

### Cache TTL Decision
```
24 hours TTL on URL cache:
  - Long enough to keep hot URLs in cache
  - Short enough that deleted URLs eventually expire
  - But we also do explicit invalidation on delete (belt + suspenders)
```

---

## 7. Async Processing & Backpressure

### Why RabbitMQ for Click Events?

```
Without queue (synchronous):
  Redirect â†’ write to MongoDB â†’ respond
  Problem: MongoDB write adds latency to every redirect

With queue (asynchronous):
  Redirect â†’ publish to RabbitMQ â†’ respond immediately
  Worker  â†’ consumes from queue  â†’ writes to MongoDB
  
  User experience: instant redirect
  Analytics: slightly delayed (seconds) â€” totally acceptable
```

### Backpressure Handling

```
Scenario: Traffic spike â†’ 10,000 clicks/second
          Worker can only process 1,000 events/second

Without backpressure:
  Queue fills up â†’ memory explodes â†’ system crashes

With backpressure (our approach):
  RabbitMQ queue has max length limit
  Worker uses prefetch count (only take N messages at a time)
  Publisher checks if queue is full before publishing
  If queue full â†’ drop event (analytics loss is acceptable)
                  OR â†’ store in fallback (Redis list)
```

```
RabbitMQ Config:
  prefetch count: 10        (worker takes 10 at a time)
  queue max length: 100,000 (after this, oldest dropped)
  message TTL: 24 hours     (stale events discarded)
```

---

## 8. Rate Limiting Design

### Strategy: Token Bucket via Redis

```
Every IP gets a bucket of 100 tokens per minute

Request comes in:
  1. GET ratelimit:{ip} from Redis
  2. If count >= 100 â†’ return 429 Too Many Requests
  3. If count < 100  â†’ INCR counter, set TTL 60s if new key, proceed

Why Redis for rate limiting?
  - Atomic INCR operation (no race conditions)
  - TTL handles automatic reset
  - Works across multiple instances (distributed)
```

### Rate Limit Tiers
```
/api/urls (create)     â†’  10 requests/min per user
/:shortcode (redirect) â†’  300 requests/min per IP
/auth/*                â†’  20 requests/min per IP
```

---

## 9. Authentication Flow

### OAuth2 Flow (Google)

```
1. User clicks "Login with Google"
2. Auth Service redirects to:
   https://accounts.google.com/oauth/authorize?client_id=...&redirect_uri=...

3. User logs in on Google

4. Google redirects back to:
   /auth/google/callback?code=AUTHORIZATION_CODE

5. Auth Service exchanges code for tokens:
   POST https://oauth2.googleapis.com/token
   Response: { access_token, id_token, ... }

6. Auth Service decodes id_token â†’ gets user info (email, name, googleId)

7. Auth Service creates/finds user in PostgreSQL

8. Auth Service issues our own JWT:
   {
     sub: userId,
     email: user.email,
     jti: uniqueTokenId,   â† for blacklisting on logout
     iat: now,
     exp: now + 7days
   }

9. JWT returned to client (in HTTP-only cookie or response body)

10. All future requests carry JWT in Authorization header
```

### JWT Blacklisting on Logout

```
Logout flow:
  1. Client sends POST /auth/logout with JWT
  2. Server extracts jti (unique token ID) from JWT
  3. Stores jti in Redis with TTL = remaining token lifetime
  4. Any future request with this JWT â†’ Redis check â†’ rejected

Why not just short expiry?
  Short expiry (15min) means users get logged out frequently.
  With blacklisting, we can have long expiry (7 days) + instant logout.
```

---

## 10. Scalability Design

### Horizontal Scaling â€” URL Service

```
                    NGINX (Load Balancer)
                   /         |          \
          URL Service   URL Service   URL Service
          Instance 1    Instance 2    Instance 3
                   \         |          /
                    PostgreSQL + Redis
                    (shared data layer)

Why this works:
  - No in-memory state on URL Service instances
  - All state lives in PostgreSQL and Redis
  - NGINX round-robins requests across instances
  - Any instance can handle any request
```

### What Makes Horizontal Scaling Possible?

```
âœ… Stateless servers        â†’ no local memory state
âœ… Shared Redis sessions     â†’ session works on any instance
âœ… Shared PostgreSQL         â†’ data consistent across instances
âœ… JWT auth                  â†’ no server-side session needed
âœ… Shared Redis rate limiter â†’ rate limits apply globally
```

### Connection Pooling

```
Without pooling:
  Each request opens a new DB connection
  PostgreSQL max connections: ~100
  1000 concurrent requests â†’ 1000 connections â†’ crash

With pooling (Prisma default):
  Pool of 10 connections shared across all requests
  Requests queue if pool is busy
  Much more efficient
  
Prisma connection pool config:
  connection_limit: 10   (per service instance)
  pool_timeout: 10       (seconds to wait for connection)
```

---

## 11. CAP Theorem in Our Stack

```
PostgreSQL â†’ CP (Consistency + Partition Tolerance)
  - Prioritizes data consistency
  - Won't return stale data
  - Used for: users, URLs (must be accurate)

Redis â†’ AP (Availability + Partition Tolerance)
  - Prioritizes availability
  - May serve slightly stale cache in edge cases
  - Used for: cache, sessions (slight staleness acceptable)

MongoDB â†’ AP (tunable)
  - Eventual consistency for analytics
  - We don't need analytics to be instantly accurate
  - Used for: click events (eventual consistency fine)
```

This is why we have **3 databases** â€” each is the right tool for its data's consistency requirements.

---

## 12. Eventual Consistency â€” Analytics

```
Click happens at T=0
  â”‚
  â–¼
Published to RabbitMQ at T=0ms
  â”‚
  â–¼
Redirect response sent at T=5ms   â† user gets redirected immediately
  â”‚
  â–¼
Worker consumes event at T=500ms  â† slight delay
  â”‚
  â–¼
Stored in MongoDB at T=520ms
  â”‚
  â–¼
Dashboard shows updated count     â† user sees it ~1 second later
```

This is eventual consistency in practice. The analytics dashboard is **not instantly accurate** but it doesn't need to be. The tradeoff is worth the redirect speed gain.

---

## 13. Environment Strategy

```
.env.development   â†’ local dev, local DBs, verbose logging
.env.test          â†’ test runner, in-memory or test DBs
.env.production    â†’ real credentials via secrets manager, minimal logging

Never commit .env files â†’ use .env.example as template
```

---

## 14. Health Checks

Every service exposes `GET /health`:

```json
{
  "status": "ok",
  "service": "url-service",
  "timestamp": "2024-01-01T00:00:00Z",
  "dependencies": {
    "postgres": "ok",
    "redis": "ok",
    "rabbitmq": "ok"
  }
}
```

Used by: Docker healthcheck, Kubernetes liveness/readiness probes, NGINX upstream health checks.

---

## 15. What We Discussed But Won't Build

| Concept | Where It Applies | How We Handle It |
|---|---|---|
| Kafka | High-throughput event streaming | Discussed theoretically; RabbitMQ used instead |
| WebSockets | Real-time dashboard updates | Polling every 5s instead |
| Vertical Scaling | Bigger server = more capacity | Discussed as Phase 1 of scaling; we go horizontal |
| CDN | Edge caching for global speed | Out of scope; mentioned as next step |
| Database sharding | Splitting DB across servers | Out of scope; discussed as future scaling step |
| Read replicas | Separate read/write DB instances | Out of scope; discussed conceptually |
